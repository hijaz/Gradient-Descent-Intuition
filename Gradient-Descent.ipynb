{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Define the function to minimize\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Define the derivative of the function\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "# Define the gradient descent algorithm\n",
    "def gradient_descent(x0, learning_rate, n_iterations):\n",
    "    x = x0\n",
    "    x_values = [x0]\n",
    "    for i in range(n_iterations):\n",
    "        x -= learning_rate * df(x)\n",
    "        x_values.append(x)\n",
    "    return x_values\n",
    "\n",
    "# Set initial values and run the algorithm\n",
    "x0 = 2.5\n",
    "learning_rate = 0.1\n",
    "n_iterations = 10\n",
    "x_values = gradient_descent(x0, learning_rate, n_iterations)\n",
    "\n",
    "# Create the animation\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-3, 3, 100)\n",
    "ax.plot(x, f(x))\n",
    "line, = ax.plot([], [], 'ro')\n",
    "\n",
    "def animate(i):\n",
    "    line.set_data(x_values[:i+1], f(np.array(x_values[:i+1])))\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=n_iterations+1, interval=500)\n",
    "\n",
    "# Display the animation\n",
    "HTML(anim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression + Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the data\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "\n",
    "# Define the cost function\n",
    "def f(theta, X, y):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "\n",
    "    # This function calculates the mean squared error between the predicted\n",
    "    # and actual values of a dependent variable given a set of coefficients (theta)\n",
    "    # and a matrix of independent variables (X)\n",
    "\n",
    "    # The division by 2m instead of m is a common convention when defining the cost function\n",
    "    # for linear regression. Dividing by 2m makes the math a bit simpler when taking the\n",
    "    # derivative of the cost function during gradient descent, as the 2 in the denominator\n",
    "    # cancels out with the 2 that comes from taking the derivative of the squared term.\n",
    "    return (1/(2*m)) * np.sum((predictions - y)**2)\n",
    "\n",
    "# Define the derivative of the cost function\n",
    "def df(theta, X, y):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    return (1/m) * (X.T.dot(predictions - y))\n",
    "\n",
    "# Define the gradient descent algorithm\n",
    "def gradient_descent(X, y, theta0, learning_rate, n_iterations):\n",
    "    theta = theta0\n",
    "    theta_values = [theta0]\n",
    "    for i in range(n_iterations):\n",
    "        theta -= learning_rate * df(theta, X, y)\n",
    "        theta_values.append(theta)\n",
    "        print(f\"Iteration {i+1}: {theta}\")\n",
    "    return theta_values\n",
    "\n",
    "# Set initial values and run the algorithm\n",
    "theta0 = np.array([0.5, 0.5])\n",
    "learning_rate = 0.1\n",
    "n_iterations = 10\n",
    "theta_values = gradient_descent(X, y, theta0, learning_rate, n_iterations)\n",
    "\n",
    "print(theta_values)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
